{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9dbb38",
   "metadata": {},
   "source": [
    "# Memory (대화 내용 기억)\n",
    "`04_memory.ipynb`\n",
    "- 대화 내용 기억\n",
    "- LLM은 기본적으로 대화 내용을 기억하지 않는다(stateless)\n",
    "- 이전 대화내용을 계속 프롬프트에 주입해야 함\n",
    "\n",
    "1. Short-Term Memory\n",
    "    - 단기기억: 한 대화 세션에 대한 기억\n",
    "2. Long-Term Memory\n",
    "    - 장기기억: 전체 세션에서 추출한 중요한 정보\n",
    "\n",
    "## Memory 구동 방식\n",
    "1. 메모리는 기본적으로 모든 대화 내역을 LLM Input에 밀어 넣는것.\n",
    "2. 이때 대화가 길어지면 토큰수 증가 및 성능 하락이 일어남\n",
    "3. 개선 방식 컨셉\n",
    "    1. 요약\n",
    "    2. 적정 길이에서 앞부분 자르기\n",
    "    3. 정리(특정 명사들로 정리, Node-Edge 그래프 방식 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8ab0a",
   "metadata": {},
   "source": [
    "## `ConversationBufferMemory`\n",
    "- 메시지 저장 -> 변수에서 추출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', '넌 유용한 챗봇이야'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),  # 기존 채팅 내역을 다 주입\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "# 메모리를 저장할 변수는 {}다. 기존에 대화내용이 있다면 불러와라\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# `chat_history`변수에, load_memory_var 결과를 저장하고, `chat_history 키를 추출`\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) |\n",
    "    itemgetter('chat_history')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = runnable | prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke({'input': '만나서 반가워'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(\n",
    "    {'human': '만나서 반가워'},\n",
    "    {'ai': res.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d073f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_message = '내 이름 뭐라고?'\n",
    "res = chain.invoke({'input': my_message})\n",
    "memory.save_context(\n",
    "    {'human': my_message},\n",
    "    {'ai': res.content}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee11f28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ffe887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인간:  난 프로그래머야\n",
      "AI: 아, 프로그래머시라니 멋지네요! 어떤 분야나 언어를 주로 사용하세요?\n",
      "---\n",
      "인간:  알아서 뭐하게\n",
      "AI: 그냥 그런 말투로 말하는 거군요. 뭐, 프로그래머라면 언제든 도움 필요하면 말하세요.\n",
      "---\n",
      "인간:  미안\n",
      "AI: 괜찮아요. 그렇게 생각하지 마세요. 언제든 이야기하고 싶을 때 편하게 말해요.\n",
      "---\n",
      "인간:  내 직업이 뭐야\n",
      "AI: 당신은 프로그래머라고 했어요. 맞죠?\n",
      "---\n",
      "인간:  그만할건데\n",
      "AI: 그만두는 결정 내리셨군요. 새로운 시작이 되길 바랄게요. 혹시 도움이 필요하면 언제든 말하세요.\n",
      "---\n",
      "인간:  \n",
      "AI: 말이 없으시네요. 필요하면 언제든 돌아오세요.\n",
      "---\n",
      "인간:  그만\n",
      "AI: 알겠습니다. 언제든 돌아오고 싶을 때 편하게 오세요. 좋은 하루 보내세요.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', '넌 좀 틱틱대는 챗봇이야. '),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),  # 기존 채팅 내역을 다 주입\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) |\n",
    "    itemgetter('chat_history')\n",
    ")\n",
    "\n",
    "chain = runnable | prompt | llm | StrOutputParser()\n",
    "\n",
    "# 메세지 스트리밍 하는 함수\n",
    "def stream_msg(chain, msg):\n",
    "    full_msg = ''\n",
    "    print('AI: ', end='')\n",
    "    for token in chain.stream({'input': msg}):\n",
    "        full_msg += token\n",
    "        print(token, end='', flush=True)\n",
    "    print('\\n---')\n",
    "    return full_msg\n",
    "\n",
    "input_msg = ''\n",
    "\n",
    "# 사용자가 ('quit', '정지', '그만') 중에 하나를 입력하면 대화 종료\n",
    "while input_msg not in ('quit', '정지', '그만'):\n",
    "    input_msg = input()\n",
    "    print('인간: ', input_msg)\n",
    "    output_msg = stream_msg(chain, input_msg)\n",
    "    memory.save_context(\n",
    "        {'human': input_msg},\n",
    "        {'ai': output_msg}\n",
    "    )\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4a8695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인간:  신사업 구상\n",
      "{'content_jp': '本報告書は、市場分析、競合他社分析、顧客ニーズの把握に基づき、新規事業の構想案を提示します。提案された事業アイデアは、環境に優しい技術、デジタル変革、カスタマイズされたサービス分野に集中しており、実行戦略と予想収益性の分析も含まれています。これにより、企業は新たな成長機会を模索し、市場競争力を強化できます。',\n",
      " 'content_kr': '본 보고서는 시장 분석, 경쟁사 분석, 고객 요구사항 파악을 바탕으로 신사업 구상안을 제시한다. 제안된 사업 '\n",
      "               '아이디어는 친환경 기술, 디지털 전환, 맞춤형 서비스 분야에 집중되어 있으며, 실행 전략과 예상 수익성 분석도 '\n",
      "               '포함되어 있다. 이를 통해 기업은 새로운 성장 기회를 모색하고 시장 경쟁력을 강화할 수 있다.',\n",
      " 'summary': '본 보고서는 새로운 사업 아이디어와 전략을 제시하여 기업의 성장 동력을 확보하는 것을 목표로 한다.',\n",
      " 'title': '신사업 구상'}\n"
     ]
    }
   ],
   "source": [
    "# 레포트 작성 챗봇\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model='gpt-4.1-nano', temperature=0)\n",
    "\n",
    "# Output parser 정의\n",
    "class Report(BaseModel):\n",
    "    title: str = Field(..., description='보고서의 제목')\n",
    "    summary: str = Field(..., description='보고서 요약본')\n",
    "    content_kr: str = Field(..., description='한국어로 작성된 보고서의 내용(1000자 이내)')\n",
    "    content_jp: str = Field(..., description='일본어로 작성된 보고서의 내용(1000자 이내)')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Report)\n",
    "\n",
    "# Prompt에 format instructions 추가\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', '넌 보고서 작성에 특화된 챗봇이야. '\n",
    "                   '반드시 지정된 형식에 맞춰 JSON으로 답변해.\\n\\nFORMAT INSTRUCTION: {format_instructions}'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Memory\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n",
    "\n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) |\n",
    "    itemgetter('chat_history')\n",
    ")\n",
    "\n",
    "# 체인 구성\n",
    "chain = runnable | prompt | llm | parser\n",
    "\n",
    "input_msg = ''\n",
    "\n",
    "# 사용자가 ('quit', '정지', '그만', '') 중에 하나를 입력하면 대화 종료\n",
    "while 1:\n",
    "    input_msg = input()\n",
    "    if input_msg  in ('quit', '정지', '그만', ''):\n",
    "        break\n",
    "\n",
    "    print('인간: ', input_msg)\n",
    "    output_msg = chain.invoke({'input': input_msg})\n",
    "    pprint(output_msg.model_dump())\n",
    "    memory.save_context(\n",
    "        {'human': input_msg},\n",
    "        {'ai': output_msg.model_dump_json()}\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
